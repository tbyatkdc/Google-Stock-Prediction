# -*- coding: utf-8 -*-
"""Proyecto1_Deep_Learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AFeaS4LTEVSGzeCBvn9U9v4XKpfIRA33

# **DEEP LEARNING**

*APLICACIÓN DE UNA RED NEURONAL RECURRENTE*

Proyecto: Google Stock Price Prediction

By:Brandon Yahir Arriaga Tlapa

El conjunto de datos consta de precios históricos de acciones de Google, con diversos atributos como precio de apertura, máximo, mínimo, cierre y volumen. Para este estudio, solo se utiliza el precio de cierre, ya que es una métrica crucial en el análisis financiero y la previsión de acciones.

# **Objetivo del Proyecto**

El objetivo principal de este proyecto es predecir el precio de cierre de las acciones de Google (GOOG) utilizando una red neuronal recurrente del tipo LSTM (Long Short-Term Memory). Para ello, se emplea un conjunto de datos históricos que abarca desde 2016 hasta 2021, centrándose en el precio de cierre como variable clave. El modelo busca capturar patrones temporales en los datos financieros, con el fin de generar predicciones precisas que puedan ser útiles para la toma de decisiones en el ámbito de inversiones y análisis bursátil. Además, se evalúa el rendimiento del modelo mediante métricas como el Error Absoluto Medio (MAE) y el Error Cuadrático Medio (RMSE).

# **Cargar y explorar datos**
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dropout, Dense
import plotly.graph_objects as go
from sklearn.metrics import mean_absolute_error, mean_squared_error

df=pd.read_csv("/content/GOOG.csv",parse_dates=["date"],index_col="date")
df.head()

df.info()

print('Valores de filas duplicadas: ' , len(df[df.duplicated()]))

plt.figure(figsize=(22,4))
sns.heatmap((df.isna().sum()).to_frame(name='df').T,cmap='GnBu', annot=True,
             fmt='0.0f').set_title('Número de valores faltantes por columna', fontsize=18)
plt.show()

"""# **Estadística descriptiva del dataset**"""

df.describe().T.style.background_gradient(cmap='GnBu',axis=1)

#Dataframe only numeric
df_numeric=df.select_dtypes(include=np.number)
df_numeric.drop(columns=["divCash","splitFactor"],inplace=True)

#Mapa de calor de correlacion
sns.heatmap(df_numeric.corr(),cmap='GnBu',annot=True)
plt.show()

plt.figure(figsize=(12, 6))
plt.plot(df['close'], color='blue', label='Precio de Cierre (GOOG)')
plt.title('Evolución del Precio de Google (2016-2021)')
plt.xlabel('Fecha')
plt.ylabel('Precio ($)')
plt.legend()
plt.grid()
plt.show()

plt.figure(figsize=(10, 5))
sns.histplot(df['close'], kde=True, bins=50, color='green')
plt.title('Distribución del Precio de Cierre')
plt.xlabel('Precio ($)')
plt.ylabel('Frecuencia')
plt.show()

df['daily_range'] = df['high'] - df['low']  # Volatilidad intradía

plt.figure(figsize=(12, 4))
plt.plot(df['daily_range'], color='purple', alpha=0.7)
plt.title('Volatilidad Diaria (High - Low)')
plt.xlabel('Fecha')
plt.ylabel('Rango ($)')
plt.show()

df['year'] = df.index.year  # Extraer el año

plt.figure(figsize=(8, 5))
sns.boxplot(x='year', y='close', data=df)
plt.title('Distribución Anual del Precio de Cierre')
plt.show()

from statsmodels.graphics.tsaplots import plot_acf

plot_acf(df['close'], lags=30)  # Autocorrelación hasta 30 días
plt.title('Autocorrelación del Precio de Cierre')
plt.show()

"""## **Implementación de la red neuronal recurrente del tipo LSTM**"""

# Cargar datos
df = pd.read_csv("/content/GOOG.csv", parse_dates=["date"], index_col="date")

# Seleccionar solo el precio de cierre
close_data = df[['close']].values

# Normalización
scaler = MinMaxScaler()
normalized_data = scaler.fit_transform(close_data)

# División train-test (80-20)
train_data, test_data = train_test_split(normalized_data, test_size=0.2, shuffle=False)

# Función para generar secuencias (solo close price)
def generate_sequences(data, seq_length=50):
    sequences = []
    labels = []

    for i in range(len(data) - seq_length):
        sequences.append(data[i:i + seq_length])
        labels.append(data[i + seq_length])

    return np.array(sequences), np.array(labels)

# Generar secuencias para entrenamiento y prueba
train_sequences, train_labels = generate_sequences(train_data)
test_sequences, test_labels = generate_sequences(test_data)

# Modelo LSTM simplificado
model = Sequential([
    LSTM(units=50, return_sequences=True, input_shape=(50, 1)),
    Dropout(0.2),
    LSTM(units=50, return_sequences=True),
    Dropout(0.2),
    LSTM(units=50),
    Dropout(0.2),
    Dense(units=1)  # Solo una salida para el precio de cierre
])

model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_absolute_error'])

# Entrenamiento
history = model.fit(
    train_sequences,
    train_labels,
    epochs=200,
    batch_size=30,
    validation_data=(test_sequences, test_labels),
    verbose=1
)

"""# **Estructura de la Red Neuronal Recurrente (LSTM)**

## **Arquitectura del Modelo**
El modelo implementado consta de las siguientes capas:
1. **Capa LSTM (50 unidades, return_sequences=True)**:  
   - Esta capa procesa secuencias de 50 pasos temporales y devuelve una secuencia completa para alimentar la siguiente capa LSTM.  
   - **Función de activación**: Tanh (por defecto en LSTM).  
   - **Dropout (0.2)**: Regularización para evitar el sobreajuste, eliminando aleatoriamente el 20% de las neuronas durante el entrenamiento.

2. **Capa LSTM (50 unidades, return_sequences=True)**:  
   - Segunda capa LSTM para profundizar en el aprendizaje de patrones temporales.  
   - **Dropout (0.2)**: Igual que la capa anterior.

3. **Capa LSTM (50 unidades)**:  
   - Última capa LSTM que devuelve solo la salida final (no secuencias).  
   - **Dropout (0.2)**: Mismo propósito de regularización.

4. **Capa Densa (1 unidad)**:  
   - Capa de salida con una neurona para predecir el precio de cierre.  
   - **Función de activación**: Lineal (para problemas de regresión).

## **Optimizador y Parámetros**
- **Optimizador**: Adam  
  - Se eligió Adam por su eficiencia en ajustar automáticamente la tasa de aprendizaje y su buen rendimiento en problemas de series temporales.  
  - **Tasa de aprendizaje**: Valor por defecto (0.001).  

- **Función de Pérdida**: Mean Squared Error (MSE)  
  - Ideal para problemas de regresión, ya que penaliza errores grandes más que los pequeños.  

- **Métricas Adicionales**: Mean Absolute Error (MAE)  
  - Proporciona una interpretación intuitiva del error en términos del precio real de las acciones.  

- **Entrenamiento**:  
  - **Épocas**: 200 (suficientes para converger sin sobreajuste).  
  - **Batch Size**: 30 (balance entre velocidad y estabilidad).  
  - **Validación**: 20% de los datos reservados para evaluar el rendimiento durante el entrenamiento.  

"""

# Predicciones
train_predictions = model.predict(train_sequences)
test_predictions = model.predict(test_sequences)

# Invertir la normalización para visualización
train_predictions = scaler.inverse_transform(train_predictions)
train_labels = scaler.inverse_transform(train_labels)
test_predictions = scaler.inverse_transform(test_predictions)
test_labels = scaler.inverse_transform(test_labels)

# Visualización
plt.figure(figsize=(16, 6))
plt.plot(train_labels, label='Actual (Train)')
plt.plot(train_predictions, label='Predicted (Train)', alpha=0.7)
plt.title('Google Stock Close Price - Training Data')
plt.xlabel('Time Steps')
plt.ylabel('Price ($)')
plt.legend()
plt.show()

plt.figure(figsize=(16, 6))
plt.plot(test_labels, label='Actual (Test)')
plt.plot(test_predictions, label='Predicted (Test)', alpha=0.7)
plt.title('Google Stock Close Price - Test Data')
plt.xlabel('Time Steps')
plt.ylabel('Price ($)')
plt.legend()
plt.show()

# Calcular métricas para test
test_mae = mean_absolute_error(test_labels, test_predictions)
test_mse = mean_squared_error(test_labels, test_predictions)
test_rmse = np.sqrt(test_mse)

print(f"Test MAE: ${test_mae:.2f}")
print(f"Test RMSE: ${test_rmse:.2f}")

plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Curva de Pérdida durante el Entrenamiento')
plt.ylabel('MSE')
plt.xlabel('Epoch')
plt.legend()
plt.show()